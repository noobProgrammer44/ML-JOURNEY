{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install tensor"
      ],
      "metadata": {
        "id": "mmLTKAA6UpbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "WFCmTOC-LKdh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_coffee_data():\n",
        "    \"\"\" Creates a coffee roasting data set.\n",
        "        roasting duration: 12-15 minutes is best\n",
        "        temperature range: 175-260C is best\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(2)\n",
        "    X = rng.random(400).reshape(-1,2)\n",
        "    X[:,1] = X[:,1] * 4 + 11.5          # 12-15 min is best\n",
        "    X[:,0] = X[:,0] * (285-150) + 150  # 350-500 F (175-260 C) is best\n",
        "    Y = np.zeros(len(X))\n",
        "\n",
        "    i=0\n",
        "    for t,d in X:\n",
        "        y = -3/(260-175)*t + 21\n",
        "        if (t > 175 and t < 260 and d > 12 and d < 15 and d<=y ):\n",
        "            Y[i] = 1\n",
        "        else:\n",
        "            Y[i] = 0\n",
        "        i += 1\n",
        "\n",
        "    return (X, Y.reshape(-1,1))"
      ],
      "metadata": {
        "id": "JwGnDfQELWSM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,y_train=load_coffee_data()\n",
        "print(X_train.shape,y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZUGtsTQP-BS",
        "outputId": "7164b5fb-389c-4744-95b1-2cc85833aedf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200, 2) (200, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Applying Normalization*\n",
        "\n",
        "Step1: Create a 'Normalization Layer'(This isnt a layer in our model)\n",
        "\n",
        "Step2: 'adapt' the data. This learns the mean and variance of the data set and saves the values internally.\n",
        "\n",
        "Step3: Normalize the data.\n",
        "\n",
        "\n",
        "**It is important to apply normalization to any future data that utilizes the learned model.**"
      ],
      "metadata": {
        "id": "eoRqstlcRvZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Data Before Normalization \\n{X_train}')"
      ],
      "metadata": {
        "id": "5d1TqoWmRO2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#In the context of tf.keras.layers.Normalization, setting axis=-1 means that the\n",
        "#normalization operation will be applied along the last axis of the input tensor\n",
        "norm_layer=tf.keras.layers.Normalization(axis=-1);\n",
        "norm_layer.adapt(X_train) #Learns Mean and Variance"
      ],
      "metadata": {
        "id": "qjdOeE4GRmIq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_norm=norm_layer(X_train)"
      ],
      "metadata": {
        "id": "ji8Z6Dy7VMEa"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Data After Normalization \\n{X_norm}')"
      ],
      "metadata": {
        "id": "612qJrb_VTEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tile/copy our data to increase the training set size and reduce the number of training epochs.\n",
        "Xt = np.tile(X_norm,(1000,1))\n",
        "Yt= np.tile(y_train,(1000,1))\n",
        "print(Xt.shape, Yt.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEaYEgRjV5ag",
        "outputId": "74b8a81e-5674-426d-da6f-bd66da0ce22d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000, 2) (200000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Building**"
      ],
      "metadata": {
        "id": "xkCPCYJmeaKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
        "model=Sequential([\n",
        "    tf.keras.Input(shape=(2,)),\n",
        "    Dense(3,activation='sigmoid',name='layer1'),\n",
        "    Dense(1,activation='sigmoid',name='layer2')\n",
        "    ])"
      ],
      "metadata": {
        "id": "euj_T4NqWAUw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note 1: The tf.keras.Input(shape=(2,)), specifies the expected shape of the input. This allows Tensorflow to size the weights and bias parameters at this point. This is useful when exploring Tensorflow models. This statement can be omitted in practice and Tensorflow will size the network parameters when the input data is specified in the model.fit statement.\n",
        "\n",
        "Note 2: Including the sigmoid activation in the final layer is not considered best practice. It would instead be accounted for in the loss which improves numerical stability.\n"
      ],
      "metadata": {
        "id": "xC9paJyGfjal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Providing a description of network\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RDZAzw4fciG",
        "outputId": "9796d184-f115-45bc-d591-10b727b49d2c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " layer1 (Dense)              (None, 3)                 9         \n",
            "                                                                 \n",
            " layer2 (Dense)              (None, 1)                 4         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13\n",
            "Trainable params: 13\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.   The model.compile statement defines a loss function and specifies a compile optimization.\n",
        "\n",
        "2.   The model.fit statement runs gradient descent and fits the weights to the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "ESz0fhLzgrm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    Xt,Yt,\n",
        "    epochs=10,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUzpHzfPgY6J",
        "outputId": "e79e0e27-0d8a-474d-871c-334f2b60d3ce"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "6250/6250 [==============================] - 7s 1ms/step - loss: 0.1780\n",
            "Epoch 2/10\n",
            "6250/6250 [==============================] - 7s 1ms/step - loss: 0.1220\n",
            "Epoch 3/10\n",
            "6250/6250 [==============================] - 7s 1ms/step - loss: 0.1011\n",
            "Epoch 4/10\n",
            "6250/6250 [==============================] - 8s 1ms/step - loss: 0.0237\n",
            "Epoch 5/10\n",
            "6250/6250 [==============================] - 9s 1ms/step - loss: 0.0132\n",
            "Epoch 6/10\n",
            "6250/6250 [==============================] - 7s 1ms/step - loss: 0.0089\n",
            "Epoch 7/10\n",
            "6250/6250 [==============================] - 8s 1ms/step - loss: 0.0063\n",
            "Epoch 8/10\n",
            "6250/6250 [==============================] - 8s 1ms/step - loss: 0.0045\n",
            "Epoch 9/10\n",
            "6250/6250 [==============================] - 7s 1ms/step - loss: 0.0032\n",
            "Epoch 10/10\n",
            "6250/6250 [==============================] - 7s 1ms/step - loss: 0.0024\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff081efbf40>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Predictions**"
      ],
      "metadata": {
        "id": "0TWi0LKshfYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.array([\n",
        "    [200,13.9],  # postive example\n",
        "    [200,17]])   # negative example\n",
        "X_testn = norm_layer(X_test)\n",
        "predictions = model.predict(X_testn)\n",
        "print(\"predictions = \\n\", predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MhPYaLpheyP",
        "outputId": "b7f042a7-03a7-42ad-b309-287d78bf7183"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 57ms/step\n",
            "predictions = \n",
            " [[9.8697132e-01]\n",
            " [4.7937636e-08]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = np.zeros_like(predictions)\n",
        "for i in range(len(predictions)):\n",
        "    if predictions[i] >= 0.5:\n",
        "        yhat[i] = 1\n",
        "    else:\n",
        "        yhat[i] = 0\n",
        "print(f\"decisions = \\n{yhat}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj9f-n5qh8t8",
        "outputId": "a4d95051-8546-4e81-9755-96f72c8e9ed1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decisions = \n",
            "[[1.]\n",
            " [0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence For [200,13.9] the coffee bean is still edible(since output is 1)\n",
        "\n",
        "while for [200,17] its not"
      ],
      "metadata": {
        "id": "Hvr64WomiJDs"
      }
    }
  ]
}